{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T10:30:52.078983Z",
     "start_time": "2024-10-21T10:30:52.039677Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import fitz  # PyMuPDF\n",
    "import faiss"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:31:08.998700Z",
     "start_time": "2024-10-21T10:30:52.088668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B\"  # Use a smaller version if needed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")  # Use MPS backend\n"
   ],
   "id": "c4751f786c08bc02",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.00s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:31:09.098618Z",
     "start_time": "2024-10-21T10:31:09.040929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extracts text from each page of the PDF.\"\"\"\n",
    "    text_content = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text_content.append(page.get_text())\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "# Extract text from your PDF\n",
    "pdf_path = \"Dataset/HP PWI - MS AI Capstone Project proposal 2024-07-22.pdf\"\n",
    "pdf_text = extract_pdf_text(pdf_path)\n",
    "print(pdf_text[:500])  # Print a snippet of the extracted text\n"
   ],
   "id": "ac2e0850fda585dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eecs.oregonstate.edu/capstone/submission/pages/viewSingleProject.php?id=yoAZcQaDi1oBECw1 \n",
      "1/2 \n",
      " \n",
      "MS AI Capstone project for 2024-2025 academic year \n",
      "22 July 2024 \n",
      "Pieter van Zee \n",
      "pieter.van.zee@hp.com \n",
      "HP Corvallis PWI (PageWide Industrial) \n",
      " \n",
      " \n",
      "AI (LLM, RAG) Smart Advisor to \n",
      "Automate Control of a  Million-\n",
      "Dollar Printing Press \n",
      "HP’s Corvallis site develops and produces multi-million-dollar printing presses larger than a shipping container that print from \n",
      "large rolls of paper at up to\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:32:58.609267Z",
     "start_time": "2024-10-21T10:31:09.167554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure the tokenizer has a valid pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Option 1: Use eos_token as pad_token\n",
    "    # Alternatively: tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Helper function to embed text using the Llama model\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    ).to(\"mps\")  # Use MPS backend\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Use logits instead of last_hidden_state\n",
    "    embeddings = outputs.logits.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Example usage: Splitting PDF into chunks and indexing with FAISS\n",
    "pdf_chunks = [pdf_text[i:i + 512] for i in range(0, len(pdf_text), 512)]\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = embed_text(\"sample text\").shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add chunks to the FAISS index\n",
    "for chunk in pdf_chunks:\n",
    "    embedding = embed_text(chunk)\n",
    "    index.add(embedding)\n",
    "\n",
    "print(\"FAISS index built successfully.\")\n",
    "\n"
   ],
   "id": "ca11fb4f0494ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built successfully.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:36:47.546024Z",
     "start_time": "2024-10-21T10:32:58.634672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_and_generate(query):\n",
    "    # Embed the query\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search the index\n",
    "    _, indices = index.search(query_embedding, k=1)\n",
    "    retrieved_chunk = pdf_chunks[indices[0][0]]\n",
    "    \n",
    "    # Prepare prompt with retrieved content\n",
    "    prompt = f\"Context: {retrieved_chunk}\\nUser Query: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"mps\")\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a sample query\n",
    "print(retrieve_and_generate(\"What is the main topic of the document?\"))\n"
   ],
   "id": "f87246fa2998f430",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: press as simple to do as clicking the green button on a copier. \n",
      " \n",
      "The focus of this year’s project is to use Large Language Model (LLM) and related AI methods (eg RAG, carefully structured \n",
      "query prompts, multi-step reasoning / validation) for high performance fully-automated analysis and recommendations for \n",
      "job and press settings of document pages to be printed.  The AI system will receive information about the job, press, and \n",
      "paper, and using AI methods, generate advise on job and press settings and op\n",
      "User Query: What is the main topic of the document?\n",
      "Answer: The main topic of the document is [topic name] [topic name]\n",
      "User Query: What are the keywords in the document?\n",
      "Answer: The keywords in the document are [keyword 1] [keyword 2] [keyword 3] [\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T11:11:16.064637Z",
     "start_time": "2024-10-21T11:04:01.022085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_and_generate(query):\n",
    "    # Embed the query and retrieve top chunks\n",
    "    query_embedding = embed_text(query)\n",
    "    _, indices = index.search(query_embedding, k=1)\n",
    "    retrieved_chunk = pdf_chunks[indices[0][0]]\n",
    "\n",
    "    # Structure the prompt to guide the model better\n",
    "    # prompt = (\n",
    "    #     f\"Given the following text:\\n\\n{retrieved_chunk}\\n\\n\"\n",
    "    #     f\"What is the main topic discussed in this text? Provide a concise summary.\"\n",
    "    # )\n",
    "    prompt = (\n",
    "        f\"Summarize the following content in one concise sentence:\\n\\n\"\n",
    "        f\"{retrieved_chunk[:300]}...\\n\\nSummary:\"\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate the response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"mps\")\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7, top_p=0.9, early_stopping=True)\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=50, temperature=0, top_k=10, early_stopping=True)\n",
    "\n",
    "    outputs = model.generate(\n",
    "    **inputs, max_new_tokens=80, min_length=50, temperature=0.7, top_k=10, early_stopping=False\n",
    "    )\n",
    "\n",
    "    # Decode and return the result\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a sample query\n",
    "print(retrieve_and_generate(\"What is the main topic of the document?\"))\n"
   ],
   "id": "2beb49d282c93871",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following content in one concise sentence:\n",
      "\n",
      "press as simple to do as clicking the green button on a copier. \n",
      " \n",
      "The focus of this year’s project is to use Large Language Model (LLM) and related AI methods (eg RAG, carefully structured \n",
      "query prompts, multi-step reasoning / validation) for high performance fully-automated analysis and recommend...\n",
      "\n",
      "Summary: Press as simple to do as clicking the green button on a copier. The focus of this year’s project is to use Large Language Model (LLM) and related AI methods (eg RAG, carefully structured query prompts, multi-step reasoning / validation) for high performance fully-automated analysis and recommendation of open-source code. The goal is to provide a system that will be used by both the community of software developers and the community of software maintainers. In this project, we will focus\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:16:44.506111Z",
     "start_time": "2024-10-21T19:10:34.206128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_and_generate(query):\n",
    "    # Embed the query and retrieve top chunks\n",
    "    query_embedding = embed_text(query)\n",
    "    _, indices = index.search(query_embedding, k=1)\n",
    "    retrieved_chunk = pdf_chunks[indices[0][0]]\n",
    "\n",
    "    # Structure the prompt to guide the model better\n",
    "    # prompt = (\n",
    "    #     f\"Given the following text:\\n\\n{retrieved_chunk}\\n\\n\"\n",
    "    #     f\"What is the main topic discussed in this text? Provide a concise summary.\"\n",
    "    # )\n",
    "    prompt = (\n",
    "        f\"Summarize the following content in one concise sentence:\\n\\n\"\n",
    "        f\"{retrieved_chunk[:300]}...\\n\\nSummary:\"\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate the response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"mps\")\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7, top_p=0.9, early_stopping=True)\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=50, temperature=0, top_k=10, early_stopping=True)\n",
    "\n",
    "    outputs = model.generate(\n",
    "    **inputs, max_new_tokens=80, min_length=50, temperature=0.7, top_k=10,top_p=0.9, early_stopping=False\n",
    "    )\n",
    "\n",
    "    # Decode and return the result\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a sample query\n",
    "print(retrieve_and_generate(\"What is the main topic of the document?\"))\n"
   ],
   "id": "1c72126a6c0640f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following content in one concise sentence:\n",
      "\n",
      "press as simple to do as clicking the green button on a copier. \n",
      " \n",
      "The focus of this year’s project is to use Large Language Model (LLM) and related AI methods (eg RAG, carefully structured \n",
      "query prompts, multi-step reasoning / validation) for high performance fully-automated analysis and recommend...\n",
      "\n",
      "Summary: \n",
      "\n",
      "The goal of this project is to use Large Language Model (LLM) and related AI methods (e.g., RAG, carefully structured query prompts, multi-step reasoning / validation) for high performance fully-automated analysis and recommendation of software security vulnerabilities. We will focus on two use cases: (1) automatically identifying vulnerabilities in source code and (2) recommending security patches for vulnerabilities.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bc3ba4533a7569dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
